{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb391af1",
   "metadata": {},
   "source": [
    "# BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cadc58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import nltk\n",
    "from transformers import BertForQuestionAnswering, AutoModelForQuestionAnswering, BertTokenizer, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "540a63a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "# model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "# Tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "# tokenizer = AutoTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93b10e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query and Context\n",
    "from queries import get_text_cli\n",
    "from get_documents import search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38fe1adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Functions\n",
    "def query_and_context():\n",
    "    term = get_text_cli('Enter a search term')\n",
    "    context = search(term)\n",
    "    query = get_text_cli(\"Enter your question\")\n",
    "    return {\n",
    "        'query': query, \n",
    "        'context_id': context[0], \n",
    "        'context_title': context[1], \n",
    "        'context': context[2]\n",
    "    }\n",
    "\n",
    "def segment_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    segments = []\n",
    "    while tokens:\n",
    "        segments.append(' '.join(tokens[:512]))\n",
    "        del tokens[:512]\n",
    "    \n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4172328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Inference\n",
    "def run_model(query, text):\n",
    "    # Initialising model\n",
    "    model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "    \n",
    "    # Initialising tokeniser\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        query,\n",
    "        text,\n",
    "        max_length=100,\n",
    "        truncation=\"only_second\",\n",
    "        stride=50,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    \n",
    "    # Running model\n",
    "    output = model(\n",
    "        input_ids=torch.tensor([inputs['input_ids'][0]]), \n",
    "        token_type_ids=torch.tensor(inputs['token_type_ids'][0])\n",
    "    )\n",
    "    \n",
    "    # Putting answer together\n",
    "    start_i = torch.argmax(output['start_logits'])\n",
    "    end_i = torch.argmax(output['end_logits'])\n",
    "    (start_i, end_i)\n",
    "    \n",
    "    answer = ' '.join(tokens[start_i:end_i+1])\n",
    "    corrected_answer = ''\n",
    "    for word in answer.split():\n",
    "        #If it's a subword token\n",
    "        if word[0:2] == '##':\n",
    "            corrected_answer += word[2:]\n",
    "        else:\n",
    "            corrected_answer += ' ' + word\n",
    "    \n",
    "    return corrected_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463f55b0",
   "metadata": {},
   "source": [
    "## Workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b1b2743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_idf import tokenize\n",
    "# word_dict = query_and_context()\n",
    "# word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e75934e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_rank(query, context, n=0):\n",
    "#     query_set = set(tokenize(query))\n",
    "#     sentences = {sent: tokenize(sent) for sent in nltk.sent_tokenize(context)}\n",
    "#     sent_scores = { sent: 0 for sent in sentences}\n",
    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    sent_scores = {\n",
    "        sent: text_similarity(query, sent, model)\n",
    "        for sent in nltk.sent_tokenize(context)\n",
    "    }\n",
    "#     for sent in sentences:\n",
    "#         common_words = query_set.intersection(set(sentences[sent]))\n",
    "#         sent_scores[sent] += len(common_words)\n",
    "    \n",
    "    ranked_scores = sorted(\n",
    "        sent_scores.items(),\n",
    "        key = lambda x: x[1],\n",
    "    )\n",
    "    \n",
    "    return ranked_scores\n",
    "\n",
    "def build_input_text(ranked_sents, max_length=512):\n",
    "    input_text = ''\n",
    "    \n",
    "    while True:\n",
    "        new_sent = ranked_sents.pop()[0]\n",
    "        if len(nltk.word_tokenize(f'{input_text} {new_sent}')) <= max_length:\n",
    "            input_text += f' {new_sent}'\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return input_text\n",
    "\n",
    "def text_similarity(text_1, text_2, model):\n",
    "    embedding_1= model.encode(text_1, convert_to_tensor=True)\n",
    "    embedding_2 = model.encode(text_2, convert_to_tensor=True)\n",
    "    \n",
    "    return float(util.pytorch_cos_sim(embedding_1, embedding_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "344e2575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_extraction_procedure():\n",
    "    word_dict = query_and_context()\n",
    "    ranked_sents = sent_rank(word_dict['query'], word_dict['context'], 0)\n",
    "    print(ranked_sents)\n",
    "    input_text = build_input_text(ranked_sents)\n",
    "    print(input_text)\n",
    "    model_output = run_model(word_dict['query'], input_text)\n",
    "    return word_dict['query'], model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8cbda94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question, answer = info_extraction_procedure()\n",
    "# print(f'Question: \"{question}\"', f'Answer: \"{answer}\"', sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1aac8e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question, answer = info_extraction_procedure()\n",
    "# print(f'Question: \"{question}\"', f'Answer: \"{answer}\"', sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601c4199",
   "metadata": {},
   "source": [
    "# OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc14a255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7baadb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_info = query_and_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06ea0bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranked_sents = sent_rank(text_info['query'], text_info['context'])\n",
    "# input_text = build_input_text(ranked_sents, 3500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6dfc141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_url = \"https://api.openai.com/v1/completions\"\n",
    "# openai.api_key = \"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\"\n",
    "\n",
    "# res = openai.Completion.create(\n",
    "#     model=\"text-davinci-003\", \n",
    "#     prompt=f\"Context: {input_text} Query: {text_info['query']}\\n\\nUsing the context, answer the query.\", \n",
    "#     temperature=0,\n",
    "# )\n",
    "# res = req.get(\n",
    "#     base_url, \n",
    "#     headers={\n",
    "#         'Authorization': f'Bearer {api_key}',\n",
    "# #         'Content-Type': 'application/json'\n",
    "#     }, \n",
    "#     data={\n",
    "#         \"model\": \"text-davinci-003\", \n",
    "#         \"prompt\": \"Say this is a test\", \n",
    "#         \"temperature\": 0, \n",
    "#         \"max_tokens\": 7,\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b778751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res.choices[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930575f7",
   "metadata": {},
   "source": [
    "# DocSearcher Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4fe8f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third-Party Imports\n",
    "import nltk\n",
    "import torch\n",
    "\n",
    "from transformers import BertForQuestionAnswering, BertTokenizer, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Standard Library Imports\n",
    "import os\n",
    "from string import punctuation\n",
    "from math import log1p, inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7688797",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocSearcher():\n",
    "    def __init__(self):\n",
    "        self._corpus = dict()\n",
    "        self._file_matches = 1\n",
    "        self._sentence_matches = 1\n",
    "    \n",
    "    def view_corpus(self):\n",
    "        return self._corpus\n",
    "\n",
    "    def load_files(self, dirname):\n",
    "        main_path = os.path.join(os.path.dirname('__file__'), dirname)\n",
    "\n",
    "        for file in os.listdir(main_path):\n",
    "            with open(os.path.join(main_path, file), 'r') as f:\n",
    "                self._corpus[file] = f.read()\n",
    "    \n",
    "    def search(self, query, s_method='tf-idf', e_method='tf-idf', fnames=None):\n",
    "        if not fnames: fnames = self._corpus.keys()\n",
    "\n",
    "        if s_method == 'tf-idf':\n",
    "            joint_context, ranked_sents = self._context_and_sents_idf(query, fnames)\n",
    "        elif s_method == 'cosine_sim':\n",
    "            joint_context, ranked_sents = self._context_and_sents_cosine(query, fnames)\n",
    "        \n",
    "#         print(joint_context, ranked_sents, sep=\"\\n\\n\")\n",
    "        \n",
    "#         print(ranked_sents[:self._sentence_matches])\n",
    "        \n",
    "        if e_method == 'conjoin':\n",
    "            output_text = self._build_output_text(ranked_sents, inf)\n",
    "            answer = ' '.join(nltk.sent_tokenize(output_text)[:self._sentence_matches])\n",
    "        elif e_method == 'bert':\n",
    "            output_text = self._build_output_text(ranked_sents, 512)\n",
    "            answer = self._run_model_bert(query, output_text)\n",
    "        elif e_method == 'openai':\n",
    "            output_text = self._build_output_text(ranked_sents, 2500)\n",
    "            answer = self._run_model_openai(query, output_text)\n",
    "        \n",
    "        print('\\n\\nAnd the output is...\\n\\n', output_text)\n",
    "        return answer\n",
    "    \n",
    "    def _build_output_text(self, ranked_sents, max_length=512):\n",
    "        output_text = ''\n",
    "\n",
    "        for sent in ranked_sents:\n",
    "            new_sent = sent[0]\n",
    "            if len(nltk.word_tokenize(f'{output_text} {new_sent}')) <= max_length:\n",
    "                output_text += f' {new_sent}'\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return output_text\n",
    "    \n",
    "    def _run_model_bert(self, query, text):\n",
    "        # Initialising model\n",
    "        model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "        # Initialising tokeniser\n",
    "        tokenizer = AutoTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            query,\n",
    "            text,\n",
    "            max_length=100,\n",
    "            truncation=\"only_second\",\n",
    "            stride=50,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_offsets_mapping=True\n",
    "        )\n",
    "\n",
    "        tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "        # Running model\n",
    "        output = model(\n",
    "            input_ids=torch.tensor([inputs['input_ids'][0]]), \n",
    "            token_type_ids=torch.tensor(inputs['token_type_ids'][0])\n",
    "        )\n",
    "\n",
    "        # Putting answer together\n",
    "        start_i = torch.argmax(output['start_logits'])\n",
    "        end_i = torch.argmax(output['end_logits'])\n",
    "        (start_i, end_i)\n",
    "\n",
    "        answer = ' '.join(tokens[start_i:end_i+1])\n",
    "        corrected_answer = ''\n",
    "        for word in answer.split():\n",
    "            #If it's a subword token\n",
    "            if word[0:2] == '##':\n",
    "                corrected_answer += word[2:]\n",
    "            else:\n",
    "                corrected_answer += ' ' + word\n",
    "\n",
    "        return corrected_answer\n",
    "    \n",
    "    def _run_model_openai(self, query, text):\n",
    "        openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "        res = openai.Completion.create(\n",
    "            model=\"text-davinci-003\", \n",
    "            prompt=f\"Context: {query} Query: {text}\\n\\nUsing only the context given, answer the query.\", \n",
    "            temperature=0,\n",
    "            max_tokens=500,\n",
    "        )\n",
    "        \n",
    "        return res.choices[0].text\n",
    "\n",
    "    def _context_and_sents_idf(self, query, fnames):\n",
    "        idfs = self._compute_idfs(fnames)\n",
    "        top_files = self._top_files_idf(query, idfs)\n",
    "\n",
    "        joint_context = \"\\n\".join(self._corpus[name] for name in top_files)\n",
    "\n",
    "        ranked_sents = self._sent_rank_idf(query, joint_context, idfs)\n",
    "\n",
    "        return joint_context, ranked_sents\n",
    "    \n",
    "    def _context_and_sents_cosine(self, query, fnames):\n",
    "        top_files = self._top_files_cosine(query, fnames)\n",
    "        joint_context = \"\\n\".join(self._corpus[name] for name in top_files)\n",
    "\n",
    "        ranked_sents = self._sent_rank_cosine(query, joint_context)\n",
    "\n",
    "        return joint_context, ranked_sents\n",
    "\n",
    "    def _cosine_similarity(self, text_1, text_2, model):\n",
    "        embedding_1= model.encode(text_1, convert_to_tensor=True)\n",
    "        embedding_2 = model.encode(text_2, convert_to_tensor=True)\n",
    "    \n",
    "        return float(util.pytorch_cos_sim(embedding_1, embedding_2))\n",
    "    \n",
    "    def _compute_idfs(self, fnames):\n",
    "        file_idfs = dict()\n",
    "        unique_words = set()\n",
    "        num_docs = len(fnames)\n",
    "\n",
    "        for name in fnames:\n",
    "            for sent in nltk.sent_tokenize(self._corpus[name]):\n",
    "                unique_words = set().union(unique_words, set(self._word_tokenize(sent)))\n",
    "                \n",
    "        for word in unique_words:\n",
    "            num_apps = sum(1 for name in fnames if word in self._corpus[name])\n",
    "            if num_apps > 0:\n",
    "                file_idfs[word] = log1p(num_docs / num_apps)\n",
    "        \n",
    "        return file_idfs\n",
    "\n",
    "    def _top_files_idf(self, query, idfs):\n",
    "        tf_idfs = { fname: 0 for fname in self._corpus }\n",
    "\n",
    "        query = self._word_tokenize(query)\n",
    "\n",
    "        for w in query:\n",
    "            for fname in self._corpus:\n",
    "                tf_idfs[fname] += self._corpus[fname].count(w) * idfs.get(w, 0)\n",
    "        \n",
    "        ranked_files = sorted(\n",
    "            tf_idfs.items(),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "\n",
    "        return [file[0] for file in ranked_files][:self._file_matches]\n",
    "    \n",
    "    def _top_files_cosine(self, query, fnames):\n",
    "        model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "        ranked_files = sorted([\n",
    "            (name, self._cosine_similarity(query, self._corpus[name], model))\n",
    "            for name in fnames\n",
    "        ], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return [file[0] for file in ranked_files][:self._file_matches]\n",
    "    \n",
    "    def _word_tokenize(self, words):\n",
    "        banned = list(punctuation) + nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "        return [\n",
    "            w.lower() for w in nltk.word_tokenize(words)\n",
    "            if w.lower() not in banned\n",
    "        ]\n",
    "    \n",
    "    def _sent_rank_idf(self, query, context, idfs):\n",
    "        query_set = set(self._word_tokenize(query))\n",
    "        sent_scores = { sent: [0,0] for sent in nltk.sent_tokenize(context)}\n",
    "\n",
    "        for sent in sent_scores:\n",
    "            sent_set = set(self._word_tokenize(sent))\n",
    "            common_words = query_set.intersection(sent_set)\n",
    "            sent_scores[sent][0] += sum(idfs.get(w, 0) for w in common_words)\n",
    "            sent_scores[sent][1] += len(common_words)\n",
    "        \n",
    "        ranked_sents = sorted(\n",
    "            sent_scores.items(),\n",
    "            key=lambda x: (x[1][0], x[1][1]),\n",
    "            reverse=True\n",
    "        )\n",
    "\n",
    "        return [(sent, score[0]) for sent, score in ranked_sents]\n",
    "\n",
    "    def _sent_rank_cosine(self, query, context):\n",
    "        model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "        sent_scores = {\n",
    "            sent: self._cosine_similarity(query, sent, model)\n",
    "            for sent in nltk.sent_tokenize(context)\n",
    "        }\n",
    "    \n",
    "        ranked_sents = sorted(\n",
    "            sent_scores.items(),\n",
    "            key = lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "    \n",
    "        return ranked_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "632de9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler = DocSearcher()\n",
    "crawler.load_files('corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3e88e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "And the output is...\n",
      "\n",
      "  It is the largest cat native to the Americas and the third largest in the world, exceeded in size only by the tiger and the lion. With a body length of up to 1.85 m (6 ft 1 in) and a weight of up to 158 kg (348 lb), it is the largest cat species in the Americas and the third largest in the world. It has powerful jaws with the third-highest bite force of all felids, after the tiger and the lion. An evaluation of JCUs from Mexico to Argentina revealed that they overlap with high-quality habitats of about 1,500 mammals to varying degrees. In South America, the jaguar is larger than the cougar and tends to take larger prey, usually over 22 kg (49 lb). The cougar's prey usually weighs between 2 and 22 kg (4 and 49 lb), which is thought to be the reason for its smaller size. An analysis of 53 studies documenting the diet of the jaguar revealed that its prey ranges in weight from 1 to 130 kg (2.2 to 286.6 lb); it prefers prey weighing 45–85 kg (99–187 lb), with capybara (Hydrochoerus hydrochaeris) and giant anteater (Myrmecophaga tridactyla) being the most selected. The tail is 45 to 75 cm (18 to 30 in) long and the shortest of any big cat. When available, it also preys on marsh deer (Blastocerus dichotomus), southern tamandua (Tamandua tetradactyla), collared peccary (Dicotyles tajacu) and black agouti (Dasyprocta fuliginosa). Its muscular legs are shorter than the legs of other Panthera species with similar body weight. Jaguars in the Chamela-Cuixmala Biosphere Reserve on the Pacific coast of central Mexico weighed around 50 kg (110 lb), which is about the size of a female cougar (Puma concolor). The jaguar is the least likely of all big cats to kill and eat humans, and the majority of attacks come when it has been cornered or wounded. Further variations in size have been observed across regions and habitats, with size tending to increase from north to south. Consumption of reptiles appears to be more frequent in jaguars than in other big cats. Jaguars in Venezuela and Brazil are much larger, with average weights of about 95 kg (209 lb) in males and of about 56–78 kg (123–172 lb) in females. Former range\n",
      "The jaguar (Panthera onca) is a large cat species and the only living member of the genus Panthera native to the Americas. Rock drawings made by the Hopi, Anasazi and Pueblo all over the desert and chaparral regions of the American Southwest show an explicitly spotted cat, presumably a jaguar, as it is drawn much larger than an ocelot.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' tiger and the lion'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawler.search('What is the biggest animal?', s_method='cosine_sim', e_method='bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83cdc3bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello world.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(nltk.sent_tokenize('Hello world. This is my story.')[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055afea0",
   "metadata": {},
   "source": [
    "# BERT Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db7a60b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_two_point_O():\n",
    "    # Load model and tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "    model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "    \n",
    "    long_text = query_and_context()['context']\n",
    "    text_len = len(long_text)\n",
    "    \n",
    "    # Chunking\n",
    "    max_length = 512\n",
    "    \n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for i in range(0, text_len, max_length):\n",
    "        chunk = long_text[i:i+max_length]\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            chunk,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "    # Stack the chunks of input IDs and attention masks\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    \n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Predict the output\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_masks)\n",
    "        start_logits, end_logits = outputs[:2]\n",
    "    \n",
    "#     # Generate answer span\n",
    "#     for i in range(len(input_ids)):\n",
    "#         # Get the start and end indices of the answer span\n",
    "#         start_ind = torch.argmax(start_logits[i])\n",
    "#         end_ind = torch.argmax(end_logits[i])\n",
    "\n",
    "#         # Use the indices to get the answer span from the input text\n",
    "#         answer_text = tokenizer.decode(input_ids[i, start_ind:end_ind + 1], skip_special_tokens=True)\n",
    "\n",
    "#         # Print the answer span\n",
    "#         print(\"Answer: \", answer_text)\n",
    "    best_answer_ind = -1\n",
    "    max_start_logit = -1e10\n",
    "    max_end_logit = -1e10\n",
    "\n",
    "    for i in range(len(input_ids)):\n",
    "        # Get the start and end logits for this chunk\n",
    "        curr_start_logit = start_logits[i].max().item()\n",
    "        curr_end_logit = end_logits[i].max().item()\n",
    "\n",
    "        # Find the chunk with the highest start and end logits\n",
    "        if curr_start_logit + curr_end_logit > max_start_logit + max_end_logit:\n",
    "            max_start_logit = curr_start_logit\n",
    "            max_end_logit = curr_end_logit\n",
    "            best_answer_ind = i\n",
    "\n",
    "    # Use the best answer indices to get the answer span from the input text\n",
    "    start_ind = torch.argmax(start_logits[best_answer_ind])\n",
    "    end_ind = torch.argmax(end_logits[best_answer_ind])\n",
    "    answer_text = tokenizer.decode(input_ids[best_answer_ind, start_ind:end_ind + 1], skip_special_tokens=True)\n",
    "\n",
    "    # Print the answer span\n",
    "    print(\"Answer: \", answer_text)\n",
    "    \n",
    "    return start_logits, end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6dc80857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a search term: Tom Cruise\n",
      "Enter your question: What was Tom Cruise's first movie?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  2001\n"
     ]
    }
   ],
   "source": [
    "start_logits, end_logits = bert_two_point_O()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3c3fb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_three_point_O():\n",
    "    # Load the pre-trained model\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "    # Define a text passage and question to be answered\n",
    "    q_and_c = query_and_context()\n",
    "    question = q_and_c['query']\n",
    "    text = q_and_c['context']\n",
    "\n",
    "    # Split the text into smaller segments\n",
    "    segment_length = 512 - 2 - len(tokenizer.tokenize(question))\n",
    "    segments = [text[i:i + segment_length] for i in range(0, len(text), segment_length)]\n",
    "\n",
    "    # Tokenize the question\n",
    "    question_tokens = tokenizer.tokenize(question)\n",
    "\n",
    "    # Initialize start and end scores for each segment\n",
    "    start_scores = []\n",
    "    end_scores = []\n",
    "\n",
    "    # Use BERT to get the start and end scores for each segment\n",
    "    for i, segment in enumerate(segments):\n",
    "        input_ids = tokenizer.encode(question, segment)\n",
    "        input_ids = torch.tensor([input_ids]).long()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            start_scores.append(outputs[0][0].squeeze()[:segment_length + 2])\n",
    "            end_scores.append(outputs[1][0].squeeze()[:segment_length + 2])\n",
    "    \n",
    "    # Pad the start and end scores so they have the same shape\n",
    "    max_len = max([s.shape[0] for s in start_scores])\n",
    "    start_scores = [torch.cat([s, torch.zeros(max_len - s.shape[0])]) for s in start_scores]\n",
    "    end_scores = [torch.cat([s, torch.zeros(max_len - s.shape[0])]) for s in end_scores]\n",
    "\n",
    "    # Combine the start and end scores for all segments\n",
    "    start_scores = torch.stack(start_scores).mean(dim=0)\n",
    "    end_scores = torch.stack(end_scores).mean(dim=0)\n",
    "\n",
    "    # Get the indices of the start and end of the answer\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores)\n",
    "\n",
    "    # Get the tokenized answer\n",
    "    answer_tokens = input_ids[0][answer_start:answer_end + 1]\n",
    "\n",
    "    # Convert the tokenized answer back to text\n",
    "    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca178ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a search term: Tom Cruise\n",
      "Enter your question: Where did Tom Cruise grow up?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_three_point_O()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f539242c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a search term: Tom Cruise\n",
      "Enter your question: Who is Tom Cruise?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_three_point_O()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f4bd447b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a search term: Nico Ditch\n",
      "Enter your question: Where is Nico Ditch?\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbert_three_point_O\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[62], line 7\u001b[0m, in \u001b[0;36mbert_three_point_O\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-large-uncased-whole-word-masking-finetuned-squad\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Define a text passage and question to be answered\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m q_and_c \u001b[38;5;241m=\u001b[39m \u001b[43mquery_and_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m question \u001b[38;5;241m=\u001b[39m q_and_c[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      9\u001b[0m text \u001b[38;5;241m=\u001b[39m q_and_c[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m, in \u001b[0;36mquery_and_context\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m context \u001b[38;5;241m=\u001b[39m search(term)\n\u001b[1;32m      5\u001b[0m query \u001b[38;5;241m=\u001b[39m get_text_cli(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter your question\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m: query, \n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext_id\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mcontext\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, \n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext_title\u001b[39m\u001b[38;5;124m'\u001b[39m: context[\u001b[38;5;241m1\u001b[39m], \n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m: context[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     11\u001b[0m }\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "bert_three_point_O()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
